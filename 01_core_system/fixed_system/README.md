# ğŸš€ Sistema AutoGen 1000

Sistema otimizado e funcional com AutoGen + Modelos Locais

## âœ… InstalaÃ§Ã£o RÃ¡pida (5 minutos)

### OpÃ§Ã£o 1: InstalaÃ§Ã£o AutomÃ¡tica (Recomendado)
```cmd
cd 01_core_system\fixed_system
install.bat
```

### OpÃ§Ã£o 2: InstalaÃ§Ã£o Manual
```cmd
# Criar ambiente virtual
py -m venv venv_autogen
venv_autogen\Scripts\activate

# Instalar dependÃªncias
pip install -r requirements.txt
```

## ğŸ¯ Como Usar

### 1. MÃ©todo Mais FÃ¡cil - Com Ollama

1. **Instale Ollama**: https://ollama.ai/download
2. **Execute o sistema**:
```cmd
python run_with_ollama.py
```

O sistema vai:
- âœ… Iniciar Ollama automaticamente
- âœ… Baixar o modelo Mistral
- âœ… Configurar tudo sozinho
- âœ… Abrir chat interativo

### 2. MÃ©todo Alternativo - Com LM Studio

1. **Instale LM Studio**: https://lmstudio.ai/
2. **Baixe um modelo** (ex: Mistral 7B GGUF)
3. **Inicie o servidor** no LM Studio (porta 1234)
4. **Execute**:
```cmd
python main.py --interactive
```

### 3. Teste RÃ¡pido
```cmd
# Testa se tudo estÃ¡ funcionando
python test_simple.py

# Testa conexÃµes
python main.py --test

# Chat direto
python main.py --chat "OlÃ¡! Como vocÃª estÃ¡?"
```

## ğŸ“ Estrutura

```
fixed_system/
â”œâ”€â”€ main.py              # Sistema principal
â”œâ”€â”€ run_with_ollama.py   # VersÃ£o fÃ¡cil com Ollama
â”œâ”€â”€ local_config.py      # Config modelos locais
â”œâ”€â”€ test_simple.py       # Teste de componentes
â”œâ”€â”€ install.bat          # Instalador automÃ¡tico
â”œâ”€â”€ requirements.txt     # DependÃªncias
â””â”€â”€ README.md           # Este arquivo
```

## ğŸ”§ SoluÃ§Ã£o de Problemas

### "AutoGen nÃ£o encontrado"
```cmd
pip install pyautogen==0.2.18
```

### "Ollama nÃ£o estÃ¡ rodando"
```cmd
# Instale Ollama primeiro
# Depois execute manualmente:
ollama serve
```

### "Erro de conexÃ£o"
- Verifique se o servidor (Ollama/LM Studio) estÃ¡ rodando
- Confirme a porta (11434 para Ollama, 1234 para LM Studio)

### "Modelo nÃ£o encontrado"
```cmd
# Para Ollama:
ollama pull mistral

# Para LM Studio:
# Baixe modelos GGUF e carregue na interface
```

## ğŸŒŸ Recursos do Sistema

- âœ… **Multi-modelo**: Suporta Ollama, LM Studio, OpenAI
- âœ… **Auto-configuraÃ§Ã£o**: Detecta e configura automaticamente
- âœ… **Modo interativo**: Chat em tempo real
- âœ… **Tolerante a falhas**: Tenta mÃºltiplas configuraÃ§Ãµes
- âœ… **Logs claros**: Mostra exatamente o que estÃ¡ acontecendo

## ğŸ’¡ Dicas Pro

1. **Performance**: Use GPU se disponÃ­vel
   ```python
   # Em local_config.py, ajuste:
   n_gpu_layers = 35  # Para GPU
   ```

2. **Modelos recomendados**:
   - Mistral 7B (equilibrado)
   - Phi-2 (rÃ¡pido, leve)
   - Llama 2 13B (mais inteligente)

3. **MÃºltiplos agentes**:
   ```python
   # Adicione mais agentes em main.py
   expert = AssistantAgent(name="expert", ...)
   ```

## ğŸ“ Suporte

Problemas? Abra o chat e digite:
```
python run_with_ollama.py --test
```

Vai mostrar exatamente o que estÃ¡ faltando!

---

**Sistema AutoGen 1000** - Feito para funcionar! ğŸš€